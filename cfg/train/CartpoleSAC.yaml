params:
  seed: ${...seed}

  algo:
    name: sac

  model:
    name: soft_actor_critic

  network:
    name: soft_actor_critic
    separate: True
    space:
      continuous:
        mu_activation: None
        sigma_activation: None

        mu_init:
          name: default
        sigma_init:
          name: orthogonal_initializer
          val: 0
        fixed_sigma: True

    mlp:
      units: [32, 32]
      activation: elu #swish

      initializer:
        name: default
      regularizer:
        name: None

    log_std_bounds: [-20, 0]


  load_checkpoint: ${if:${...checkpoint},True,False} # flag which sets whether to load the checkpoint
  load_path: ${...checkpoint} # path to the checkpoint to load

  config:
    name: ${resolve_default:Cartpole,${....experiment}}
    full_experiment_name: ${.name}
    env_name: rlgpu
    num_envs: ${....task.env.numEnvs} # Number of envs running in parallel
    total_steps: 4e6  # Max number of experience samples
    batch_size: 2048  # Batch size for training critic and actor networks from replay buffer data
    device: cuda
    score_to_win: 20000
    print_stats: True

    # Algorithmic parameters
    init_alpha: 1
    num_seed_steps: 5000     # Number of sim steps with random policy actions.
    max_episode_steps: inf   # Episodes never end on timeout
    replay_buffer_size: 1000000
    reward_shaper:
      scale_value: 1.0
    mixed_precision: true
    normalize_input: False
    target_entropy_coef: 1.0   # Leave at 1.0 to comply with Arnoha
    # Optimization parameters
    alpha_lr: 3e-4
    actor_lr: 3e-4
    critic_lr: 3e-4
    critic_tau: 5e-3  # Double Q network weight update rate
    learnable_temperature: true
    gamma: 0.990

    # Logging parameters
    log_frequency: 5e4    # Experience samples
    save_frequency: 1e6  # Experience samples
    save_best_after_n_steps: 1e6  # Experience samples
